{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba3338fd",
   "metadata": {},
   "source": [
    "# TUTORIAL 5 - TRAINING SPIKING NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82f23b",
   "metadata": {},
   "source": [
    "In this tutorial, we will train a Spiking Neural Network (SNN) on the MNIST dataset. We will discuss the limitation of training SNNs and how to overcome them using surrogate gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f3e3b",
   "metadata": {},
   "source": [
    "## THE NON-DIFFERENTIABLE OF SPIKES\n",
    "Because the membrane potential is affected by previous spikes, SNNs resemble recurrent neural networks (RNNs). However, the non-differentiable nature of spikes makes training SNNs more challenging.\n",
    "\n",
    "$$\\begin{align*}\n",
    "    U[t+1] = \\beta U[t] + WX[t+1]-R[t]\n",
    "\\end{align*}$$\n",
    "\n",
    "Where $R[t]$ is the reset mechnism, that defaults to substraction (subtratcting the threshold $U_{th}$)\n",
    "\n",
    "$$\\begin{align*}\n",
    "    S[t] = \\begin{cases}\n",
    "        1 & \\text{if } U[t] > U_{th} \\\\\n",
    "        0 & \\text{o.w.}\n",
    "    \\end{cases}\n",
    "\\end{align*}$$\n",
    "\n",
    "$S[t]$ can be represeted as the derivative of the Heaviside step function, which is not differentiable. \n",
    "$$\\begin{align*}\n",
    "    S[t] = \\Theta(U[t] - U_{th}) \n",
    "\\end{align*}$$\n",
    "\n",
    "The derivative of the Heaviside step function equals infinity at the threshold, and zero otherwise - the Dirac Delta function. Because we take the derivative during backpropagation step (chain rule), the gradient of the loss function is zero everywhere except at the threshold, where it is infinite. This means that no learning can take place. To solve this problem, we are going to make a small modification: during the `forward pass`, we will use the Heaviside step function, but during the `backward pass`, we will approximate the step function with a continuous function that has a non-zero derivative everywhere. This is called a *surrogate function*. It works pretty well in practice, and we will use it in this tutorial.\n",
    "\n",
    "An example of a surrogate function is $\\arctan$, with the following derivative:\n",
    "$$\\begin{align*}\n",
    "    \\frac{d}{dx} \\arctan(x) = \\frac{1}{1+x^2}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee137b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakySurrogate(nn.Module):\n",
    "    \"\"\"\n",
    "    A surrogate function for the Heaviside step function.\n",
    "    It is used to approximate the step function during backpropagation.\n",
    "\n",
    "    Arguments:\n",
    "        beta (float): Decay rate of the surrogate function.\n",
    "        threshold (float): Threshold value for the surrogate function.\n",
    "            Default is 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta, threshold=1.0):\n",
    "        super(LeakySurrogate, self).__init__()\n",
    "        \n",
    "        # initialize decay rate and threshold\n",
    "        self.beta = beta #torch.tensor(beta, requires_grad=False, device=device)\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.spike_gradient = self.ATan.apply   # We are going to rewrite the arctan function of pytorch\n",
    "\n",
    "    def init_leaky(self, batch_size, num_neurons):\n",
    "        return torch.zeros((batch_size, num_neurons), dtype=dtype).to(device)\n",
    "\n",
    "    def forward(self, input_, mem):\n",
    "        \"\"\"Called each time we call LeakySurrogate\"\"\"\n",
    "\n",
    "        spk = self.spike_gradient((mem-self.threshold))\n",
    "        reset = (self.beta*spk*self.threshold).detach()     # Remove reset from computation graph\n",
    "        mem = self.beta * mem + input_ - reset\n",
    "        return spk, mem\n",
    "\n",
    "    # Now lets rewrite the forward and backward methods of arctan\n",
    "    # Forward pass: Heaviside step function\n",
    "    # Backward pass: Override Dirac Delta function with derivative of arctan function\n",
    "    \n",
    "    @staticmethod\n",
    "    class ATan(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, mem):\n",
    "            spk = (mem > 0).float()         # Heaviside on the forward pass\n",
    "            ctx.save_for_backward(mem)      # Store the membrane for use in the backward pass\n",
    "            return spk\n",
    "        \n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            (mem, ) = ctx.saved_tensors   # Retrieve the stored membrane potential\n",
    "            # Use torch.pi for device compatibility and multiply by grad_output\n",
    "            grad = 1/(1 + (torch.pi * mem).pow(2)) * grad_output\n",
    "            return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aede8a3",
   "metadata": {},
   "source": [
    "Now lets instansiate the leaky neuron model with the surrogate function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4466c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "lif1 = LeakySurrogate(beta=0.9)\n",
    "lif1.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43836b",
   "metadata": {},
   "source": [
    "## SETTING UP THE STATIC MNIST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datalader arguments\n",
    "batch_size = 128\n",
    "data_path = '/tmp/data/mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed40717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deine input transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),        # Resize images to 28x28\n",
    "    transforms.Grayscale(),             # Convert images to grayscale\n",
    "    transforms.ToTensor(),              # Convert images to tensors\n",
    "    transforms.Normalize((0.,), (1.,))  # Normalize images to [0, 1]\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750132aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f9a4c",
   "metadata": {},
   "source": [
    "## DEFINE THE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 28*28      # flattened size of MNIST images\n",
    "num_hidden = 1000\n",
    "num_outputs = 10        # 10 classes for MNIST\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25          # Num of time steps in each spamle\n",
    "beta = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742fed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SNN network\n",
    "class SNNnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = LeakySurrogate(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = LeakySurrogate(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Init membrane potentials\n",
    "        mem1 = self.lif1.init_leaky(x.shape[0], self.fc1.out_features)   # set membrane potential to 0       \n",
    "        mem2 = self.lif2.init_leaky(x.shape[0], self.fc2.out_features)   # set membrane potential to 0\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # stimulate the network for a time window (num of steps)\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            \n",
    "            # record the spikes and membrane potential\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "    \n",
    "# Load the network\n",
    "net = SNNnet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760edee",
   "metadata": {},
   "source": [
    "## LOSS DEFINITION\n",
    "We are going to use softmax on the output membrane potential, followed by cross entropy loss - the correct neuron representing the class should have the highest potential. Using `torch.nn.CrossEntropyLoss` will take care of the softmax and the loss calculation for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8033fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c09a96",
   "metadata": {},
   "source": [
    "## OPTIMIZER\n",
    "\n",
    "Adam is a robust optimizer that performs wee on recurrent networks, so let's use that with a learning rate of $5 \\times 10^{-4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))  # Use beta values for Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2313cbc",
   "metadata": {},
   "source": [
    "## ACCURACY METRIC\n",
    "\n",
    "Below are functions that take a batch of data, counts up all the spikes from each neruon (equivilent to rate code over sim time), and compares the index of the highes count with actual target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the data into the network, sum spikes over time.\n",
    "# Compare the neuro with the hgiest number of spikes with the target.\n",
    "\n",
    "\n",
    "def get_batch_accuracy(net_output, targets):\n",
    "    num_spikes = net_output.sum(dim=0)  # Sum spikes over time\n",
    "    _, predicted_idx = torch.max(num_spikes, 1)  # Get the index of the max\n",
    "    acc = np.mean((targets == predicted_idx).detach().cpu().numpy())\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e408ef",
   "metadata": {},
   "source": [
    "## SANITY TESTS\n",
    "Lets load the first batch of data, load it onto CUDA and check that we understand the sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "data = data.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "# Now lets run a single pass through the network.\n",
    "# Important - this is a fully connected nn (not conv), so we need to flatten the input data\n",
    "\n",
    "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "print(\"Spikes shape [Time_Steps, Samples_of_Data, Labels]:\", spk_rec.shape)\n",
    "print(\"Membrane shape [Time_Steps, Samples_of_Data, Labels]:\", mem_rec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bcff1",
   "metadata": {},
   "source": [
    "We got:\n",
    "* 25 Time Steps\n",
    "* 128 Samples of Data\n",
    "* 10 Labels\n",
    "\n",
    "Now lets see what the loss on the dataset is, using the network we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init validation loss\n",
    "train_loss = torch.zeros(1, dtype=dtype).to(device)\n",
    "\n",
    "# This is the validation loss, taken over the first batch of data, over time_steps of the time window.\n",
    "for step in range(num_steps):\n",
    "    train_loss += loss(mem_rec[step], targets)        # Validation loss is computed over the membrane potential\n",
    "\n",
    "print(f\"Train Loss: {train_loss.item()}\")\n",
    "print(f\"Train Accuracy: {get_batch_accuracy(spk_rec, targets)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a5951",
   "metadata": {},
   "source": [
    "The train loss is summed over all time steps. In correspondance, the accuracy is also low. This is because the network is not trained yet.\n",
    "\n",
    "Now lets run a single backward pass, update the weights and check the loss and accuracy again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, init the optimizer to remove all previous gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Calculate the gradients\n",
    "train_loss.backward()\n",
    "\n",
    "# Update the weights\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1e67c",
   "metadata": {},
   "source": [
    "Now lets compute the loss again, on the same batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "# initiazlize the loss\n",
    "train_loss = torch.zeros(1, dtype=dtype).to(device)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    train_loss += loss(mem_rec[step], targets)        # Validation loss is computed over the membrane potential\n",
    "\n",
    "print(f\"Train Loss: {train_loss.item()}\")\n",
    "print(f\"Train Accuracy: {get_batch_accuracy(spk_rec, targets)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb8d1b9",
   "metadata": {},
   "source": [
    "The accuracy has increased from 7.81% to 11.72%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20b8f2",
   "metadata": {},
   "source": [
    "# FULL TRAINING LOOP\n",
    "Now lets run the training again and see what happens to the loss and accuracy. We are going to train the network for 10 epochs, and save the loss and accuracy at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dfd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    counter = 0\n",
    "    for data, targets in train_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))   # Shape is [Time_Steps, Samples_of_Data, Labels]\n",
    "\n",
    "        # Go through all time steps. \n",
    "        # For each time step, calculate the loss and accuracy\n",
    "        # The loss is calculated over the membrane potential\n",
    "        # The accuracy is calculated over the spikes\n",
    "\n",
    "        train_loss = torch.zeros(1, dtype=dtype).to(device)  # Reset the loss for each batch\n",
    "        for step in range(num_steps):\n",
    "            train_loss += loss(mem_rec[step], targets)       # The membrane potential should be highest for the correct class\n",
    "            \n",
    "        # batch accuracy\n",
    "        acc = get_batch_accuracy(spk_rec, targets)\n",
    "        train_loss_history.append(train_loss.item())\n",
    "        train_acc_history.append(acc)   # Train accuracy per batch\n",
    "        \n",
    "        if counter % 100 == 0:\n",
    "            # Print the loss and accuracy every 100 batches\n",
    "            print(f\"Train \\t Epoch {epoch+1}/{num_epochs},\\t Loss: {train_loss.item():.4f}, \\t Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()  # Zero the gradients before the backward pass\n",
    "        train_loss.backward()\n",
    "        optimizer.step()  # Update the weights\n",
    "\n",
    "        counter += 1\n",
    "    \n",
    "    # After each epoch, we can validate the model on the test set\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = torch.zeros(1, dtype=dtype).to(device)\n",
    "        val_acc = []\n",
    "\n",
    "        for data, targets in test_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "            batch_val_loss = torch.zeros(1, dtype=dtype).to(device)\n",
    "            for step in range(num_steps):\n",
    "                batch_val_loss += loss(mem_rec[step], targets)\n",
    "\n",
    "            batch_val_loss /= len(test_loader)\n",
    "            batch_val_acc = get_batch_accuracy(spk_rec, targets)\n",
    "\n",
    "            val_loss += batch_val_loss\n",
    "            val_acc_history.append(batch_val_acc)\n",
    "\n",
    "            val_loss_history.append(batch_val_loss.item())  # Average loss over the test set\n",
    "\n",
    "\n",
    "        print(f\"Val \\t Epoch {epoch+1}/{num_epochs},\\t Loss: {val_loss.item():.4f}, \\t Accuracy: {np.mean(val_acc_history)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e09a38",
   "metadata": {},
   "source": [
    "# PLOTTING THE RESULTS\n",
    "Now lets plot the loss and accuracy over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plott loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Batches')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plott accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_acc_history, label='Train Accuracy')\n",
    "plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy over Batches')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417394d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
